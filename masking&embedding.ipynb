{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c456408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# embedding\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10a6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>score</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi, my name is Amy.</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>Amy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi, my name is Kate.</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>Kate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi, my name is Emily.</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>Emily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi, my name is Sam.</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>Sam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi, my name is Sarah.</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>Sarah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi, my name is Claire.</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>Claire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hi, my name is Julie.</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>Julie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hi, my name is Alice.</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi, my name is Mia.</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>Mia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hi, my name is Emma.</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>Emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hi, my name is Charlie.</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>Charlie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hi, my name is Anna.</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>Anna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hi, my name is Rachel.</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hi, my name is David.</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>David</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hi, my name is Grace.</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>Grace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sequence     score token_str\n",
       "0       Hi, my name is Amy.  0.008801       Amy\n",
       "1      Hi, my name is Kate.  0.007974      Kate\n",
       "2     Hi, my name is Emily.  0.007432     Emily\n",
       "3       Hi, my name is Sam.  0.007135       Sam\n",
       "4     Hi, my name is Sarah.  0.006704     Sarah\n",
       "5    Hi, my name is Claire.  0.006109    Claire\n",
       "6     Hi, my name is Julie.  0.006059     Julie\n",
       "7     Hi, my name is Alice.  0.005701     Alice\n",
       "8       Hi, my name is Mia.  0.005455       Mia\n",
       "9      Hi, my name is Emma.  0.004981      Emma\n",
       "10  Hi, my name is Charlie.  0.004834   Charlie\n",
       "11     Hi, my name is Anna.  0.004733      Anna\n",
       "12   Hi, my name is Rachel.  0.004436    Rachel\n",
       "13    Hi, my name is David.  0.004413     David\n",
       "14    Hi, my name is Grace.  0.004383     Grace"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize MLM pipeline\n",
    "mlm = pipeline('fill-mask', model='bert-base-cased', top_k=15)\n",
    "\n",
    "# get mask token\n",
    "mask = mlm.tokenizer.mask_token\n",
    "\n",
    "# get result for particular masked phrase\n",
    "def predict(phrase):\n",
    "    \n",
    "    predicted = mlm(phrase.format(mask))\n",
    "    result = pd.DataFrame(predicted)\n",
    "    result.drop('token', axis=1, inplace=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "predict(\"Hi, my name is {}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0e6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# embed word using sentence context\n",
    "# much of the code in this section comes from Arushi Prakash; credits in bib doc\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-cased', output_hidden_states = True)\n",
    "\n",
    "# set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def prepare_sent(sent):\n",
    "\n",
    "    marked_text = \"[CLS] \" + sent + \" [SEP]\"\n",
    "\n",
    "    # tokenize sentence with BERT tokenizer.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors):\n",
    "    \n",
    "    # gradient calculation id disabled, model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # get embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    \n",
    "    # collapse the tensor into 1 dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    \n",
    "    # convert torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings\n",
    "\n",
    "def embed_target(sent, word):\n",
    "    \n",
    "    tokenized_text, tokens_tensor, segments_tensors = prepare_sent(sent)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    # find position of word in list of tokens\n",
    "    if word in tokenized_text:\n",
    "        word_index = tokenized_text.index(word)\n",
    "    else:\n",
    "        print('FLAG')\n",
    "        return 1\n",
    "    \n",
    "    # get the embedding for word\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "        \n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6250b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange the embedding around 2 new dimensions\n",
    "# much of the code in this section comes from Ted Underwood; credits in bib doc\n",
    "\n",
    "def get_vectors(sent_word_pairs):\n",
    "    \n",
    "    vectors = []\n",
    "    \n",
    "    for sent, targ in sent_word_pairs.items():          # for each word in a wordlist\n",
    "        vec = embed_target(sent, targ)\n",
    "        vectorlength = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors.append(vec / vectorlength)              # and save it in a list\n",
    "        \n",
    "    thesum = np.sum(vectors, axis = 0)                  # then add all the vectors\n",
    "    vectorlength = np.linalg.norm(thesum, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum / vectorlength\n",
    "\n",
    "def make_direction(positive_examples, negative_examples):\n",
    "    \n",
    "    posvector = get_vectors(positive_examples)           # create a mean positive vector\n",
    "    negvector = get_vectors(negative_examples)           # and negative vector\n",
    "    direction = posvector - negvector                    # subtract the second from the first\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def plot_the_frame(plotframe):   \n",
    "    \n",
    "    plt.figure(figsize = (10, 8))\n",
    "    theplot = sns.scatterplot(x = plotframe.iloc[ : , 0], y = plotframe.iloc[ : , 1])\n",
    "    collabels = plotframe.columns.tolist()\n",
    "    theplot.set(xlabel = collabels[0], ylabel = collabels[1])\n",
    "\n",
    "    for rownum in range(plotframe.shape[0]):\n",
    "        x = plotframe.iloc[rownum, 0]\n",
    "        y = plotframe.iloc[rownum, 1]\n",
    "        theplot.text(x + 0.007, y + .007, \n",
    "        plotframe.index[rownum], horizontalalignment='left', \n",
    "        size='medium', color='black', weight='semibold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def organizeby2dims(sent_word_pairs, ref_vector1, ref_vector2, col1name, col2name):\n",
    "    \n",
    "    dict4df = dict()\n",
    "    words = list(sent_word_pairs.values())\n",
    "    \n",
    "    for colname in [col1name, col2name]:      # create a dictionary with \n",
    "        dict4df[colname] = []                 # appropriate key names\n",
    "        \n",
    "    for s, w in sent_word_pairs.items():      # for each word       \n",
    "        vec = embed_target(s, w)\n",
    "        cos1 = cosine(vec, ref_vector1)       # get its x position\n",
    "        cos2 = cosine(vec, ref_vector2)       # and y position\n",
    "        \n",
    "        dict4df[col1name].append(cos1)\n",
    "        dict4df[col2name].append(cos2)\n",
    "\n",
    "    plotframe = pd.DataFrame(dict4df, index = words)  # make this a data frame,\n",
    "                                                      # with words as index labels\n",
    "    plot_the_frame(plotframe)\n",
    "    \n",
    "    return plotframe\n",
    "\n",
    "animate = {'I feel completely alive.' : 'alive',\n",
    "           'He is my friend.' : 'friend',\n",
    "           'A man must do his best.' : 'man',\n",
    "           'If anyone can do it, he can.' : 'he',\n",
    "           'If anyone can do it, she can' : 'she',\n",
    "           'I am a human being' : 'human',\n",
    "           'As he is a person, he is granted rights.' : 'person',\n",
    "           'Humans are intelligent creatures' : 'intelligent',\n",
    "           'Nothing is more important in life than family.' : 'family'}\n",
    "\n",
    "inanimate = {'I cannot use it.' : 'it',\n",
    "             'When an animal is dead, it no longer feels any pain.' : 'dead',\n",
    "             'I turned the rock over to see what was inside' : 'rock',\n",
    "             'Get that thing away from me.' : 'thing',\n",
    "             'Be careful when you lift that heavy object.' : 'object',\n",
    "             'The trees are planted on irrigated dirt and the fruit gathered between November and August.' : 'dirt',\n",
    "             'These laws are intended to help preserve our natural resources.' : 'resources'}\n",
    "\n",
    "animacy = make_direction(inanimate, animate)\n",
    "\n",
    "human = {'there is nothing more powerful than a human' : 'human'}\n",
    "nonhuman = {'i turned a stone over to see what was underneath it.' : 'stone'}\n",
    "\n",
    "humanness = make_direction(nonhuman, human)\n",
    "\n",
    "def zip_and_embed(sent_list, word_list):\n",
    "    \n",
    "    zipped = dict(zip(sent_list, word_list))\n",
    "    plotframe = organizeby2dims(zipped, animacy, humanness, 'animacy', 'humanness')\n",
    "    \n",
    "    return plotframe['animacy'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9be364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG3 %\n",
      "FLAG6 %\n",
      "FLAG4 %\n",
      "FLAG2 %\n",
      "FLAG %\n",
      "FLAG4 %\n",
      "FLAG8 %\n",
      "FLAG7 %\n",
      "FLAG9 %\n",
      "FLAG5 %\n",
      "FLAG3 %\n",
      "FLAG\n",
      "FLAG1 %\n",
      "FLAG9 %\n",
      "FLAG3 %\n",
      "FLAG %%\n",
      "FLAG8 %\n",
      "FLAG6 %\n",
      "FLAG %%\n",
      "FLAG6 %\n",
      "FLAG8 %\n",
      "FLAG3 %\n",
      "FLAG5 %\n",
      "FLAG3 %\n",
      "100%2 %\n",
      "FLAG7 %\n",
      "100%7 %\n"
     ]
    }
   ],
   "source": [
    "# run the pipeline\n",
    "\n",
    "def pipeline(directory, result_file):\n",
    "    \n",
    "    # construct master frame\n",
    "    frame = pd.read_csv(directory, sep='\\t', index_col=[0])\n",
    "    \n",
    "    # create columns; must be done to pre-set the type\n",
    "    frame['prediction'] = 0\n",
    "    frame['full_prediction'] = 0\n",
    "    frame['probability'] = 0\n",
    "    frame['animacy'] = 0\n",
    "    \n",
    "    # pre-set column type as object to accommodate lists\n",
    "    frame['prediction'] = frame['prediction'].astype(object)\n",
    "    frame['full_prediction'] = frame['full_prediction'].astype(object)\n",
    "    frame['probability'] = frame['probability'].astype(object)\n",
    "    frame['animacy'] = frame['animacy'].astype(object)\n",
    "    \n",
    "    # extract masked sentences for prediction\n",
    "    sentences = frame['MaskedSentence'].to_list()\n",
    "    \n",
    "    # make predictions\n",
    "    length = len(frame)\n",
    "    for x in range(0, length):\n",
    "        \n",
    "        sentence = sentences[x]\n",
    "\n",
    "        sentence_predict_df = predict(sentence)\n",
    "        prediction = sentence_predict_df['token_str'].to_list()\n",
    "        full_prediction = sentence_predict_df['sequence'].to_list()\n",
    "        probabilities = sentence_predict_df['score'].to_list()\n",
    "\n",
    "        frame.at[x, 'prediction'] = prediction\n",
    "        frame.at[x, 'full_prediction'] = full_prediction\n",
    "        frame.at[x, 'probability'] = probabilities\n",
    "    \n",
    "        #Run embedding\n",
    "        full_predictions = frame['full_prediction'].to_list()[x]\n",
    "        predictions = frame['prediction'].to_list()[x]\n",
    "        animacy_embeddings = zip_and_embed(full_predictions, predictions)\n",
    "        \n",
    "        frame.at[x, 'animacy'] = animacy_embeddings\n",
    "    \n",
    "        # weight animacy with probability\n",
    "        frame.at[x, 'avg_animacy'] = np.average(animacy_embeddings,weights=probabilities)\n",
    "        #frame.to_csv('/home/dustin/Python/Animacies/results/test.csv')\n",
    "        percentage = round(x / length * 100, 2)\n",
    "        print(percentage, '%', end='\\r')\n",
    "    \n",
    "    print('100%')\n",
    "        \n",
    "    # reorder frame by weight and save\n",
    "    final_results = frame.sort_values('avg_animacy')\n",
    "    final_results.to_csv('/home/dustin/Python/Animacies/results/' + result_file + '.csv')\n",
    "\n",
    "    return None\n",
    "\n",
    "pipeline('/home/dustin/Python/Animacies/filtered/bryophytes.tsv', 'bryophytes')\n",
    "pipeline('/home/dustin/Python/Animacies/filtered/sweetgrass.tsv', 'sweetgrass')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
